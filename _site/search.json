[
  {
    "objectID": "key_findings.html",
    "href": "key_findings.html",
    "title": "Key Findings",
    "section": "",
    "text": "This analysis examines 2,046 CalFresh applications submitted via GetCalFresh.org in San Diego County. The goal was to understand which factors are most strongly associated with approval outcomes and where the process might be improved to better serve eligible applicants."
  },
  {
    "objectID": "key_findings.html#factors-associated-with-approval",
    "href": "key_findings.html#factors-associated-with-approval",
    "title": "Key Findings",
    "section": "1. Factors Associated With Approval",
    "text": "1. Factors Associated With Approval\nTo identify which factors were most strongly associated with CalFresh approval, a logistic regression model was used. The outcome variable was whether an application was approved. Predictors included self-reported income, household composition, housing stability, interview completion (self-reported), and document submission behavior (before and after application). Variables were selected based on program relevance, user behavior, and exploratory analysis.\nAll numeric variables were checked for correlation and scaled for interpretability (e.g., income was divided by $500). Missing data were minimal except for had_interview, which was recoded as a categorical variable with an “unknown” level to avoid excluding many applicants. The model achieved a McFadden R² of 0.15 and an AUC of 0.76 — indicating good fit and predictive performance using only application-facing data.\nKey Results and Interpretations\nThe table below summarizes the model results, with both statistical detail and a plain-language interpretation for each variable. This format supports both researchers and stakeholders.\n\n\n\n\n\n\n\n\nVariable\nOdds Ratio\n95% CI (Low)\n95% CI (High)\nP-Value\nInterpretation\n\n\n\n\nBaseline (reference)\n2.02\n1.51\n2.71\n0.00\nBaseline odds (intercept)\n\n\nIncome (per $500)\n0.67\n0.62\n0.71\n0.00\nHigher income was linked to lower approval\n\n\nHousehold Size\n0.83\n0.68\n1.01\n0.07\nLarger households were not significantly different\n\n\nChildren in Household\n1.46\n1.13\n1.89\n0.00\nEach child increased odds of approval\n\n\nOlder Adults in Household\n1.20\n0.85\n1.70\n0.29\nNo significant effect\n\n\nDocs Submitted With Application\n1.13\n1.07\n1.19\n0.00\nEach document increased approval odds by ~12%\n\n\nDocs Submitted After Application\n1.07\n1.01\n1.14\n0.02\nEach document had a small positive effect\n\n\nApplication Time (minutes)\n1.00\n0.99\n1.00\n0.20\nLonger applications showed no strong association\n\n\nStable Housing\n0.90\n0.70\n1.15\n0.38\nNo clear difference after accounting for other factors\n\n\nInterview Completed\n2.99\n2.31\n3.89\n0.00\nStrongest predictor of approval\n\n\n\n\n\n\n\n\nSummary:\n\nInterview completion was the strongest predictor of approval. Applicants who confirmed completing the interview had a predicted approval rate of ~72%, compared to 50% for others.\nDocument submission with the application improved outcomes. Each document uploaded increased approval odds.\nHigher income reduced approval odds, consistent with eligibility thresholds.\nHaving children on the application increased approval likelihood, while housing stability and application duration had no significant impact after adjusting for other variables."
  },
  {
    "objectID": "key_findings.html#potential-improvements",
    "href": "key_findings.html#potential-improvements",
    "title": "Key Findings",
    "section": "2. Potential Improvements",
    "text": "2. Potential Improvements\nThe biggest opportunities for improvement appear after the application is submitted. Applicants who didn’t confirm an interview or didn’t submit documents were much less likely to be approved — even when income-eligible.\n\nSummary:\n\nHelp applicants complete interviews with reminders, pre-scheduling options, or simplified confirmation workflows.\nEncourage early document uploads by guiding users to upload verification documents during the application flow, especially when they’re likely to qualify.\nExplore geographic disparities in approval outcomes across ZIP codes, potentially linked to caseloads or infrastructure (e.g., broadband access).\n\nThese targeted improvements could reduce drop-off among eligible users and make the approval process more efficient and equitable."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "For questions or additional information, feel free to contact:\nMari Roberts\nProject Author\nEmail: marialexandriaroberts@gmail.com\nGitHub: @mr4909"
  },
  {
    "objectID": "contact.html#team",
    "href": "contact.html#team",
    "title": "Contact",
    "section": "",
    "text": "For questions or additional information, feel free to contact:\nMari Roberts\nProject Author\nEmail: marialexandriaroberts@gmail.com\nGitHub: @mr4909"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CalFresh SNAP Application Analysis",
    "section": "",
    "text": "Welcome to the CalFresh SNAP Application Analysis site, created as part of a data science take home assignment for Code for America.\nThe analysis explores factors associated with CalFresh (SNAP) application approvals in San Diego County, based on ~2,000 applications submitted via GetCalFresh.org.\nThis project aims to surface key patterns in approval outcomes and suggest potential improvements to the application process."
  },
  {
    "objectID": "about.html#introduction",
    "href": "about.html#introduction",
    "title": "CalFresh SNAP Application Analysis",
    "section": "",
    "text": "Welcome to the CalFresh SNAP Application Analysis site, created as part of a data science take home assignment for Code for America.\nThe analysis explores factors associated with CalFresh (SNAP) application approvals in San Diego County, based on ~2,000 applications submitted via GetCalFresh.org.\nThis project aims to surface key patterns in approval outcomes and suggest potential improvements to the application process."
  },
  {
    "objectID": "about.html#objectives",
    "href": "about.html#objectives",
    "title": "CalFresh SNAP Application Analysis",
    "section": "Objectives",
    "text": "Objectives\n\nIdentify which applicant characteristics are most strongly associated with approval.\nUnderstand where the process may break down (e.g., interviews, document submission).\nSuggest actionable, user-centered improvements to reduce friction in the process."
  },
  {
    "objectID": "about.html#key-questions",
    "href": "about.html#key-questions",
    "title": "CalFresh SNAP Application Analysis",
    "section": "Key Questions",
    "text": "Key Questions\n\nWhat factors are most strongly associated with CalFresh approval?\nWhere would you look next to improve the application process?"
  },
  {
    "objectID": "about.html#project-links",
    "href": "about.html#project-links",
    "title": "CalFresh SNAP Application Analysis",
    "section": "Project Links",
    "text": "Project Links\n\nGitHub Repository: Link to repo\nExercise Prompt: View exercise PDF\nData Source: Download from Google Drive"
  },
  {
    "objectID": "about.html#stakeholders",
    "href": "about.html#stakeholders",
    "title": "CalFresh SNAP Application Analysis",
    "section": "Stakeholders",
    "text": "Stakeholders\nThis analysis models the collaborative work that would involve:\n\nCode for America’s GetCalFresh team: Program designers and policy advocates.\nCounty Human Services Agencies: Decision-makers and data providers.\nApplicants: The core user base, especially those facing structural barriers to access."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis Walkthrough",
    "section": "",
    "text": "This analysis explores which factors are associated with whether a CalFresh (SNAP) application is approved or denied. The data come from 2,046 applications submitted through GetCalFresh.org in San Diego County.\n\nOur goal is to understand:\n\nWhat factors are associated with whether an applicant is approved or denied\nWhere in the process users may drop off, face barriers, or get delayed\nHow the application experience could be improved for users\n\n\nThis is not a causal analysis. Instead, it aims to show practical insights about patterns in approval outcomes, using variables tied to both applicant eligibility and their interactions with the process."
  },
  {
    "objectID": "analysis.html#overview",
    "href": "analysis.html#overview",
    "title": "Analysis Walkthrough",
    "section": "",
    "text": "This analysis explores which factors are associated with whether a CalFresh (SNAP) application is approved or denied. The data come from 2,046 applications submitted through GetCalFresh.org in San Diego County.\n\nOur goal is to understand:\n\nWhat factors are associated with whether an applicant is approved or denied\nWhere in the process users may drop off, face barriers, or get delayed\nHow the application experience could be improved for users\n\n\nThis is not a causal analysis. Instead, it aims to show practical insights about patterns in approval outcomes, using variables tied to both applicant eligibility and their interactions with the process."
  },
  {
    "objectID": "analysis.html#application-walkthrough",
    "href": "analysis.html#application-walkthrough",
    "title": "Analysis Walkthrough",
    "section": "Application Walkthrough",
    "text": "Application Walkthrough\nBefore analyzing the data, I walked through the GetCalFresh.org application process myself. This helped clarify how each field is presented to applicants, what steps are required or optional, and where friction might arise.\n\n\n\nCalFresh Survey Image\n\n\nSeeing the application from a user’s perspective provided critical context for interpreting the dataset — especially for process-related variables like document uploads, interview completion, and application duration.\n\nKey Takeaways\n\nMultilingual support is available early in the application (English, Spanish, Chinese, Vietnamese), with additional language preferences captured later.\nThe application is structured in stages: household info → income → expenses → contact details → confirmation.\nApplicants get real-time feedback about possible eligibility and whether they might qualify for expedited processing.\nDocument uploads and interviews aren’t required at submission — they can happen later, which means missing data doesn’t always signal ineligibility.\nToward the end, applicants confirm contact information, set preferences (e.g., language, reminders), and indicate interview availability.\n\nThis walkthrough was useful for interpreting behavioral data in context — especially steps that are optional, delayed, or invisible in the dataset (like phone interviews completed outside the platform)."
  },
  {
    "objectID": "analysis.html#about-the-data",
    "href": "analysis.html#about-the-data",
    "title": "Analysis Walkthrough",
    "section": "About the Data",
    "text": "About the Data\nThis dataset includes 2,046 CalFresh (SNAP) applications submitted through GetCalFresh.org in San Diego County. Each row represents one application and contains:\n\nInformation reported by the applicant\nActivity tracked through the GetCalFresh platform\nFinal approval outcome provided by the county\n\nThe dataset reflects the user-facing side of the process. It does not capture every factor a county worker might see (e.g., paper documents or offline interviews), but it provides a detailed view of what users did on the site and what happened afterward.\nKey Variables\n\n\n\n\n\n\n\n\nVariable\nDescription\nNotes\n\n\n\n\nincome\nHousehold income in the last 30 days\nSlightly randomized for privacy\n\n\nhousehold_size\nNumber of people applying\nUsed to determine income thresholds\n\n\ndocs_with_app\nDocuments uploaded with the application\nOptional at time of submission\n\n\ndocs_after_app\nDocuments uploaded after submission\nOften submitted after the interview\n\n\nhad_interview\nApplicant’s self-report of completing the interview\nBased on SMS follow-up; may be missing\n\n\ncompletion_time_mins\nTime taken to complete the application\nMay include pauses or returns\n\n\nstable_housing\nWhether applicant rents/owns their sleeping location\nProxy for housing stability\n\n\nunder18_n, over_59_n\nChildren or older adults in the household\nMay influence prioritization or eligibility\n\n\nzip\nApplicant ZIP code\nMay reflect local conditions or access issues\n\n\napproved\nFinal approval outcome\nProvided by the county\n\n\n\nContextual Notes:\n\nInterview completion (had_interview) comes from an SMS response. Missing values do not confirm whether an interview occurred — only that no reply was recorded.\nDocument fields only include uploads through GetCalFresh. Submissions made by mail, fax, or in person are not tracked here.\nApproval decisions come from county records and represent real outcomes.\n\nUnderstanding what’s captured — and what’s missing — helped guide how I handled missingness and interpreted variables."
  },
  {
    "objectID": "analysis.html#exploratory-data-analysis",
    "href": "analysis.html#exploratory-data-analysis",
    "title": "Analysis Walkthrough",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore modeling, I conducted an exploratory analysis to:\n\nUnderstand the distribution and structure of each variable\nIdentify missingness and data quality issues\nSpot early signals related to approval\nPrepare features for interpretability and modeling\n\n\nCodebook Summary\nI created a structured codebook using a function from my own databookR package. It describes each variable, its type, missingness, and key statistics — and forms the foundation for all further steps.\n\n# Add descriptions to variables\nvar_desc &lt;- list(\n  app_id               = \"Unique identifier for each application\",\n  completion_time_mins = \"Time taken to complete the application, in minutes\",\n  household_size       = \"Number of people applying for CalFresh in the household\",\n  income               = \"Total household income in the last 30 days (randomized slightly for privacy)\",\n  docs_with_app        = \"Count of verification documents uploaded with the initial application\",\n  docs_after_app       = \"Count of verification documents uploaded after application (via Later Docs)\",\n  under18_n            = \"Number of children age 17 or younger included in the application\",\n  over_59_n            = \"Number of adults age 60 or older included in the application\",\n  stable_housing       = \"TRUE if applicant rents or owns the place they sleep; FALSE otherwise\",\n  had_interview        = \"TRUE if applicant reported completing the required interview; may be missing\",\n  zip                  = \"ZIP code where the applicant lives or stays\",\n  approved             = \"TRUE if the application was approved for CalFresh by the county\"\n)\n\n# Generate databook\ndatabookR::databook(exercise_data, var_descriptions = var_desc)\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nVariable Description\nVariable Type\nNumber of Unique Values\nPercentage Missing\nStatistics\n\n\n\n\napp_id\nUnique identifier for each application\nnumeric\n2046\n0.0%\nMin: 1.0\nAvg: 38866.9\nMedian: 38066.5\nMax: 110550.0\nSD: 25305.4\n\n\ncompletion_time_mins\nTime taken to complete the application, in minutes\nnumeric\n2046\n0.0%\nMin: 2.3\nAvg: 21.4\nMedian: 10.4\nMax: 8551.8\nSD: 195.7\n\n\nhousehold_size\nNumber of people applying for CalFresh in the household\nnumeric\n10\n0.2%\nMin: 1.0\nAvg: 1.8\nMedian: 1.0\nMax: 12.0\nSD: 1.3\n\n\nincome\nTotal household income in the last 30 days (randomized slightly for privacy)\nnumeric\n888\n0.0%\nMin: 0.0\nAvg: 936.0\nMedian: 270.0\nMax: 9779.0\nSD: 1228.9\n\n\ndocs_with_app\nCount of verification documents uploaded with the initial application\nnumeric\n19\n0.0%\nMin: 0.0\nAvg: 1.3\nMedian: 0.0\nMax: 25.0\nSD: 2.2\n\n\ndocs_after_app\nCount of verification documents uploaded after application (via Later Docs)\nnumeric\n20\n0.0%\nMin: 0.0\nAvg: 0.8\nMedian: 0.0\nMax: 29.0\nSD: 2.1\n\n\nunder18_n\nNumber of children age 17 or younger included in the application\nnumeric\n7\n0.2%\nMin: 0.0\nAvg: 0.5\nMedian: 0.0\nMax: 6.0\nSD: 1.0\n\n\nover_59_n\nNumber of adults age 60 or older included in the application\nnumeric\n3\n0.2%\nMin: 0.0\nAvg: 0.1\nMedian: 0.0\nMax: 2.0\nSD: 0.3\n\n\nstable_housing\nTRUE if applicant rents or owns the place they sleep; FALSE otherwise\nlogical\n2\n0.0%\nTRUE: 1186 (58.0%)\nFALSE: 860 (42.0%)\n\n\nhad_interview\nTRUE if applicant reported completing the required interview; may be missing\nlogical\n2\n49.7%\nTRUE: 595 (57.8%)\nFALSE: 434 (42.2%)\n\n\nzip\nZIP code where the applicant lives or stays\nfactor\n28\n0.0%\nTop 5 Categories:\n92105 = 162 (7.9%)\n92114 = 157 (7.7%)\n92113 = 155 (7.6%)\n92115 = 133 (6.5%)\n92154 = 128 (6.3%)\n\n\napproved\nTRUE if the application was approved for CalFresh by the county\nlogical\n2\n0.0%\nTRUE: 1148 (56.1%)\nFALSE: 898 (43.9%)\n\n\n\n\n\n\n\nNotable Patterns:\n\nIncome is low for most applicants (median = $270/month).\nApplication time is short (median = 10 minutes), but some outliers take hours.\nMost users submit no documents online, even after applying.\nInterview data is missing for more than half — not necessarily because no interview occurred.\nApproval rate = 56%, giving enough variation for modeling.\n\nThis codebook gave me a clear view of how people move through the process. It highlighted variables with missing or unusual values, flagged behaviors worth looking into (like how few applicants upload documents or report completing interviews), and pointed to patterns that might be important for understanding who gets approved.\n\n\nMissingness\nTo assess data quality, I visualized missingness across all variables using vis_miss().\n\nvis_miss(exercise_data)\n\n\n\n\n\n\n\n\nOnly had_interview has meaningful missing data (~50%). All other fields are complete or nearly complete. This missingness is expected — the interview variable comes from a follow-up SMS survey, and not all applicants respond.\n\n\nDistribution of Key Variables\nI next reviewed distributions of numeric variables to check for skew, outliers, and interpretability issues.\n\n# Custom binwidths\nbinwidths &lt;- list(\n  income = 250,\n  completion_time_mins = 40,\n  docs_with_app = 1,\n  docs_after_app = 1,\n  household_size = 1,\n  under18_n = 1,\n  over_59_n = 1\n)\n\n# Generate all plots\nwrap_plots(\n  fnc_plot_var(\"income\", \"Monthly Income\"),\n  fnc_plot_var(\"completion_time_mins\", \"App Completion Time (Minutes)\", max_x = 120),\n  fnc_plot_var(\"docs_with_app\", \"Docs Uploaded With App\"),\n  fnc_plot_var(\"docs_after_app\", \"Docs Uploaded After App\"),\n  fnc_plot_var(\"household_size\", \"Household Size\"),\n  fnc_plot_var(\"under18_n\", \"Children in Household\"),\n  fnc_plot_var(\"over_59_n\", \"Older Adults in Household\"),\n  ncol = 2\n)\n\n\n\n\n\n\n\n\nObservations:\n\nIncome is heavily right-skewed. Most applicants report less than $500/month.\nCompletion time clusters below 20 minutes. A few outliers extend far beyond that range.\nDocument uploads: The majority of applicants upload no documents at all, either before or after applying.\nHousehold size: Most applicants apply alone or with one other person.\nChildren and older adults: Most applications list zero dependents under 18 or over 59.\n\nThese patterns suggest that most applicants are low-income, likely applying alone, and often do not upload documents upfront — all of which could influence approval outcomes.\n\n\nCorrelation and Redundancy Check\nBefore modeling, I examined relationships among predictors to identify potential multicollinearity or redundancy. This ensures that coefficients remain stable and interpretable.\nMethod 1: Correlation Matrix\nI computed pairwise correlations among numeric variables:\n\n# Select numeric predictors\nnum_vars &lt;- exercise_data |&gt;\n  select(income, household_size, under18_n, over_59_n, \n         docs_with_app, docs_after_app, completion_time_mins)\n\n# Compute correlations\ncor_matrix &lt;- cor(num_vars, use = \"complete.obs\")\n\n# Visualize\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         tl.cex = 0.8, tl.col = \"black\",\n         col = colorRampPalette(c(cfa_colors$red, \"white\", cfa_colors$blue))(200))\n\n\n\n\n\n\n\n\nInterpretation:\n\nunder18_n and household_size are moderately correlated (makes sense — families with children are larger).\nAll other variables are weakly correlated.\nNo pairs appear highly redundant.\n\nMethod 2: Variance Inflation Factor (VIF)\nI fit a preliminary logistic regression model to compute VIFs:\n\n# Quick VIF check with basic logistic model\nvif_model &lt;- glm(\n  approved ~ income + household_size + under18_n + over_59_n +\n    docs_with_app + docs_after_app + completion_time_mins + \n    stable_housing + had_interview,\n  data = exercise_data,\n  family = binomial()\n)\n\ncar::vif(vif_model)\n\n              income       household_size            under18_n \n            1.570242             4.949500             4.810768 \n           over_59_n        docs_with_app       docs_after_app \n            1.082801             1.071075             1.094315 \ncompletion_time_mins       stable_housing        had_interview \n            1.012481             1.241863             1.105751 \n\n\n\nhousehold_size (4.95) and under18_n (4.81) show moderate multicollinearity — expected due to their conceptual overlap.\nAll other variables have VIFs below 2.\n\nBoth variables household_size and under18_n, will likely be retained. While correlated, they reflect different eligibility factors: household size affects income limits, while the presence of children may affect processing or priority.\n\n\nApproval Rates by Key Variables\nTo understand where in the process outcomes start to diverge, I looked at approval rates across key variables. This helped identify patterns worth modeling and showed possible intervention points.\n\n\nInterview Completion\n\nfnc_approval_summary(exercise_data, had_interview)\n\n\n\n\n\n\n\nGroup\nN\nApproval Rate (%)\n\n\n\n\nFALSE\n434\n50.0\n\n\nTRUE\n595\n72.1\n\n\nNA\n1,017\n49.4\n\n\n\n\n\n\n\n\nApplicants who reported completing the interview had higher approval rates than those who did not or whose response was missing.\nThis reinforces the interview as a critical point of potential drop-off.\nMissing responses likely indicate no follow-up engagement — not necessarily ineligibility.\n\n\n\nDocument Submission Group\nI grouped applicants by when (or whether) they submitted documents:\n\n# Add a doc_group variable \nexercise_data &lt;- exercise_data |&gt; \n  mutate(\n    doc_group = case_when(\n        docs_with_app &gt; 0 & docs_after_app == 0 ~ \"With App Only\",\n        docs_with_app == 0 & docs_after_app &gt; 0 ~ \"After App Only\",\n        docs_with_app &gt; 0 & docs_after_app &gt; 0 ~ \"With + After\",\n        docs_with_app == 0 & docs_after_app == 0 ~ \"No Docs\"\n        ) |&gt; \n      factor(levels = c(\"With App Only\", \"After App Only\", \"With + After\", \"No Docs\"))\n      )\nfnc_approval_summary(exercise_data, doc_group)\n\n\n\n\n\n\n\nGroup\nN\nApproval Rate (%)\n\n\n\n\nWith App Only\n797\n65.0\n\n\nAfter App Only\n248\n57.3\n\n\nWith + After\n213\n65.7\n\n\nNo Docs\n788\n44.2\n\n\n\n\n\n\n\n\nApproval was highest among applicants who submitted documents with their initial application.\nApplicants who submitted documents only after applying had moderately lower rates.\nThe lowest approval rate (~44%) was among those who submitted nothing online.\n\nThis suggests that early document submission may signal follow-through — or speed up processing.\n\n\nHousing Stability\n\nfnc_approval_summary(exercise_data, stable_housing)\n\n\n\n\n\n\n\nGroup\nN\nApproval Rate (%)\n\n\n\n\nFALSE\n860\n64.4\n\n\nTRUE\n1,186\n50.1\n\n\n\n\n\n\n\n\nApplicants reporting unstable housing had higher approval rates.\nThis may reflect prioritized eligibility for those experiencing housing instability.\nHousing instability may increase likelihood of approval due to expedited or simplified eligibility pathways.\n\n\n\nHousehold Size (Binned)\nI binned household size for interpretability:\n\n# Add bins\nexercise_data &lt;- exercise_data |&gt; \n  mutate(household_size_bin = cut(household_size, breaks = c(0,1,2,3,5,10)))\n\nfnc_approval_summary(exercise_data, household_size_bin)\n\n\n\n\n\n\n\nGroup\nN\nApproval Rate (%)\n\n\n\n\n(0,1]\n1,223\n61.7\n\n\n(1,2]\n334\n52.1\n\n\n(2,3]\n227\n45.4\n\n\n(3,5]\n228\n46.1\n\n\n(5,10]\n29\n27.6\n\n\nNA\n5\n60.0\n\n\n\n\n\n\n\n\n1–2 person households had the highest approval rates.\nLarger households saw lower approval rates.\nThis may be due to stricter income thresholds at higher household sizes — or more complexity in verifying eligibility.\n\n\n\nZIP Code Variation\nZIP code can reflect structural factors that influence access: geography, internet connectivity, support, and even worker caseloads. While it’s not causal, it helps show system-level variation.\nI assessed variation in approval rates by ZIP code, filtering out ZIPs with fewer than 10 applications:\n\n# Aggregate by ZIP (filter out sparse ZIPs)\nzip_summary &lt;- exercise_data |&gt;\n  group_by(zip) |&gt;\n  summarize(\n    n = n(),\n    approval_rate = mean(approved, na.rm = TRUE)\n  ) |&gt;\n  filter(n &gt;= 10)\n\n# Bar chart\nggplot(zip_summary, aes(x = fct_reorder(zip, approval_rate), \n                        y = approval_rate * 100)) +\n  geom_col(fill = cfa_colors$blue) +\n  coord_flip() +\n  labs(\n    title = \"Approval Rate by ZIP Code (≥10 applications)\",\n    x = \"ZIP Code\",\n    y = \"Approval Rate (%)\"\n  ) +\n  fnc_theme_cfa()\n\n\n\n\n\n\n\n\nNotes:\n\nApproval rates vary from ~34% to ~69% across ZIP codes.\nThis range is large enough to suggest systematic differences, not just noise.\nHigh- and low-performing ZIPs each have reasonable sample sizes, supporting this concern.\n\n\n\nStatistical Test: Is ZIP Predictive of Approval?\nTo formally test whether ZIP code is predictive of approval, I ran a chi-squared test:\n\nzip_test &lt;- exercise_data |&gt;\n  filter(!is.na(approved), !is.na(zip)) |&gt;\n  count(zip, approved) |&gt;\n  pivot_wider(names_from = approved, values_from = n, values_fill = 0) |&gt;\n  column_to_rownames(\"zip\") |&gt;\n  as.matrix() |&gt;\n  chisq.test()\n\nzip_test\n\n\n    Pearson's Chi-squared test\n\ndata:  as.matrix(column_to_rownames(pivot_wider(count(filter(exercise_data,     !is.na(approved), !is.na(zip)), zip, approved), names_from = approved,     values_from = n, values_fill = 0), \"zip\"))\nX-squared = 43.85, df = 27, p-value = 0.02143\n\n\nInterpretation:\n\nThe chi-squared test was statistically significant (p &lt; 0.05).\nWe reject the null hypothesis that approval rates are independent of ZIP code.\nEven without socioeconomic indicators, ZIP appears to meaningfully stratify outcomes.\n\n\n\nPreparation for Modeling\nBefore fitting a model, I created new variables and adjusted a few existing ones to improve interpretability. These changes were grounded in earlier exploratory analysis and CalFresh eligibility rules.\nThe goal was to make model coefficients easier to interpret and ensure alignment with how eligibility and case processing work in practice.\nIncome\n\nexercise_data &lt;- exercise_data |&gt;\n  mutate(income_500 = income / 500)\n\n\nScaled income in $500 units to make coefficients easier to interpret.\nA log-odds change now reflects the effect of each additional ~$500 in monthly income, not each dollar.\n\nApplication Completion Time\n\nexercise_data &lt;- exercise_data |&gt;\n  mutate(\n    completion_time_capped = if_else(completion_time_mins &gt; 180, 180, completion_time_mins),\n    long_app = completion_time_mins &gt; 60\n  )\n\n\nCapped extreme values above 180 minutes to reduce the of outliers.\nFlagged apps that took more than one hour (long_app) to explore whether interruptions or complexity affect outcomes.\n\nInterview Completion (Self-Reported)\n\nexercise_data &lt;- exercise_data |&gt;\n  mutate(interview_completed = case_when(\n    had_interview == TRUE ~ \"Completed\",\n    TRUE ~ \"Not confirmed\"\n  ) |&gt; factor(levels = c(\"Not confirmed\", \"Completed\")))\n\n\nRe-coded had_interview into a 2-category factor:\n\n“Completed” = applicant said they had the interview\n“Not confirmed” = didn’t respond or said no\n\nThis avoids misinterpreting missing data as a definitive “no”"
  },
  {
    "objectID": "analysis.html#income-eligibility",
    "href": "analysis.html#income-eligibility",
    "title": "Analysis Walkthrough",
    "section": "Income Eligibility",
    "text": "Income Eligibility\nTo better understand who should be approved under CalFresh rules, I used the official income eligibility thresholds based on household size.\nThese limits reflect the 200% Federal Poverty Level under California’s Broad-Based Categorical Eligibility (BBCE) policy.\nThis allows us to distinguish:\n\nApplicants who likely met income-based eligibility\nApplicants who may have been denied despite being income-eligible\nThe extent to which approval decisions align with income thresholds\n\n\n# Create eligibility table based on the table here:\n# https://dpss.lacounty.gov/en/food/calfresh/gross-income.html\neligibility_table &lt;- tibble::tibble(\n  household_size = 1:8,\n  max_gross_income = c(2510, 3408, 4304, 5200, 6098, 6994, 7890, 8788)\n)\n\n# Join with application data\nexercise_data &lt;- exercise_data |&gt;\n  left_join(eligibility_table, by = \"household_size\") |&gt;\n  mutate(\n    income_eligible = income &lt;= max_gross_income\n  )\n\n# Approval summary by income eligibility\napproval_summary &lt;- exercise_data |&gt;\n  group_by(income_eligible) |&gt;\n  summarize(\n    n = n(),\n    approval_rate = mean(approved, na.rm = TRUE),\n    approved_over_income = sum(approved & !income_eligible, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    approval_rate = round(approval_rate * 100, 1)\n  )\n\n# Highlight minimum approval rate\nmin_rate &lt;- min(approval_summary$approval_rate, na.rm = TRUE)\n\napproval_summary |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    income_eligible = \"Income-Eligible?\",\n    n = \"N\",\n    approval_rate = \"Approval Rate (%)\",\n    approved_over_income = \"Approved Despite High Income\"\n  ) |&gt;\n  gt::fmt_number(columns = c(n, approved_over_income), decimals = 0) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\nIncome-Eligible?\nN\nApproval Rate (%)\nApproved Despite High Income\n\n\n\n\nFALSE\n43\n9.3\n4\n\n\nTRUE\n1,997\n57.1\n0\n\n\nNA\n6\n50.0\n0\n\n\n\n\n\n\n\nTakeaways:\n\nMost applicants appear income-eligible.\nA small number were approved despite exceeding income thresholds — potentially due to exceptions, data entry errors, or income verification.\nNearly half of income-eligible applicants were not approved, pointing to process barriers like missed interviews or incomplete documentation.\n\nThis flag helps contextualize approval decisions in the model, especially when eligible applicants are denied."
  },
  {
    "objectID": "analysis.html#logistic-regression",
    "href": "analysis.html#logistic-regression",
    "title": "Analysis Walkthrough",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nTo identify factors most strongly associated with CalFresh approval, I fit a logistic regression model. This model estimates the likelihood of approval based on eligibility-related variables and applicant actions observed through the application process.\n\nVariable Selection Rationale\nI included variables based on:\n\nProgram relevance (e.g., income, household size)\nUser experience (e.g., document upload, interview)\nResults from earlier exploratory analysis\n\nThe final model uses:\n\nScaled income: income_500\nHousehold composition: household_size, under18_n, over_59_n\nDocument submission: docs_with_app, docs_after_app\nTime spent applying: completion_time_capped\nHousing stability: stable_housing\nInterview completion: interview_completed (re-coded)\n\n\n\nModel Specification\n\napproval_model &lt;- glm(\n  approved ~ income_500 + household_size + under18_n + over_59_n +\n    docs_with_app + docs_after_app + completion_time_capped +\n    stable_housing + interview_completed,\n  data = exercise_data,\n  family = binomial()\n)\n\nsummary(approval_model)\n\n\nCall:\nglm(formula = approved ~ income_500 + household_size + under18_n + \n    over_59_n + docs_with_app + docs_after_app + completion_time_capped + \n    stable_housing + interview_completed, family = binomial(), \n    data = exercise_data)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   0.628432   0.131272   4.787 1.69e-06 ***\nincome_500                   -0.412618   0.029816 -13.839  &lt; 2e-16 ***\nhousehold_size               -0.125768   0.088893  -1.415  0.15712    \nunder18_n                     0.300545   0.117048   2.568  0.01024 *  \nover_59_n                     0.121020   0.155961   0.776  0.43777    \ndocs_with_app                 0.116576   0.025326   4.603 4.16e-06 ***\ndocs_after_app                0.075872   0.027076   2.802  0.00507 ** \ncompletion_time_capped       -0.001958   0.002632  -0.744  0.45674    \nstable_housingTRUE           -0.086942   0.112313  -0.774  0.43887    \ninterview_completedCompleted  1.098309   0.118276   9.286  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2800.1  on 2041  degrees of freedom\nResidual deviance: 2373.5  on 2032  degrees of freedom\n  (4 observations deleted due to missingness)\nAIC: 2393.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nInterpretation:\n\nInterview completed had the strongest association with approval.\nHigher income reduced the odds of approval, as expected.\nDocument uploads (both with and after application) were positively associated with approval.\nEach additional child was associated with higher approval odds.\nOther variables (e.g., housing stability, household size, older adults, app time) were not significant once the above were accounted for.\n\n\n\nOdds Ratios and Confidence Intervals\n\nmodel_results &lt;- broom::tidy(approval_model, exponentiate = TRUE, conf.int = TRUE)\n\nmodel_results |&gt;\n  select(term, estimate, conf.low, conf.high, p.value) |&gt;\n  mutate(across(where(is.numeric), round, 2)) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    term = \"Variable\",\n    estimate = \"Odds Ratio\",\n    conf.low = \"95% CI (Low)\",\n    conf.high = \"95% CI (High)\",\n    p.value = \"P-Value\"\n  ) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\nVariable\nOdds Ratio\n95% CI (Low)\n95% CI (High)\nP-Value\n\n\n\n\n(Intercept)\n1.87\n1.45\n2.43\n0.00\n\n\nincome_500\n0.66\n0.62\n0.70\n0.00\n\n\nhousehold_size\n0.88\n0.74\n1.05\n0.16\n\n\nunder18_n\n1.35\n1.07\n1.70\n0.01\n\n\nover_59_n\n1.13\n0.83\n1.53\n0.44\n\n\ndocs_with_app\n1.12\n1.07\n1.18\n0.00\n\n\ndocs_after_app\n1.08\n1.02\n1.14\n0.01\n\n\ncompletion_time_capped\n1.00\n0.99\n1.00\n0.46\n\n\nstable_housingTRUE\n0.92\n0.74\n1.14\n0.44\n\n\ninterview_completedCompleted\n3.00\n2.38\n3.79\n0.00\n\n\n\n\n\n\n\n\nAn odds ratio &gt; 1 means the variable is associated with a higher chance of approval\nAn odds ratio &lt; 1 means a lower chance\n\nInterpretation:\n\nInterview completed: ~3x higher odds of approval\nEach $500 in income: ~34% lower odds\nEach document uploaded with the application: ~12% higher odds\nEach child (under 18): ~35% higher odds\n\nThese results reinforce earlier descriptive findings — but also show that certain variables (like app duration or housing status) have little added explanatory value once core factors are controlled for."
  },
  {
    "objectID": "analysis.html#diagnostics",
    "href": "analysis.html#diagnostics",
    "title": "Analysis Walkthrough",
    "section": "Diagnostics",
    "text": "Diagnostics\nAfter fitting the logistic regression, I ran several checks to evaluate how well the model fits the data and whether the results are trustworthy. These diagnostics focus on:\n\nHow much variation the model explains\nWhether predicted probabilities align with actual outcomes\nHow well the model distinguishes approved vs. denied applications\n\n1. McFadden’s Psuedo R²\nDefinition: A measure of how much better the model fits the data compared to a model with no predictors (just an intercept).\n\n# Pseudo R-squared\npscl::pR2(approval_model)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-1186.7634616 -1400.0644571   426.6019909     0.1523508     0.1885348 \n         r2CU \n    0.2526548 \n\n\nThe pseudo R² was around 0.15, which indicates a moderate effect size. That’s typical in behavioral data, where many influencing factors aren’t captured in the dataset.\n2. Hosmer–Lemeshow Goodness-of-Fit Test\nThis test checks whether the model’s predicted probabilities align with the actual outcomes. It groups observations into deciles by predicted probability, then compares predicted vs. actual approval rates in each group.\n\nhoslem.test(approval_model$y, fitted(approval_model))\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  approval_model$y, fitted(approval_model)\nX-squared = 9.0452, df = 8, p-value = 0.3385\n\n\nOur p-value is 0.34, which is not staistically significant. That’s good — it means there’s no evidence of poor fit. The model’s predictions are consistent with observed data.\n3. ROC Curve and AUC (Area Under the Curve)\nAUC summarizes how well the model distinguishes between approved and denied applicants.\n\nmodel_data &lt;- model.frame(approval_model)\nactual &lt;- model_data$approved\npredicted &lt;- fitted(approval_model)\n\nroc_obj &lt;- roc(actual, predicted)\nplot(roc_obj, col = cfa_colors$blue, lwd = 2)\n\n\n\n\n\n\n\npROC::auc(roc_obj)\n\nArea under the curve: 0.7563\n\n\nInterpretation:\n\nAUC = 0.76 means the model assigns a higher predicted probability to an approved case than a denied one 76% of the time.\nThat’s considered good performance for a logistic model using only observable application behaviors.\n\nTogether, these diagnostics show that the model is well-calibrated, explains meaningful variation, and performs reliably — even with known limitations in the dataset.\n4. Train/Test Split\nTo evaluate how well the model generalizes, I randomly split the data into:\n\n80% training set (used to fit the model)\n20% test set (used to evaluate performance on unseen data)\n\nThe model was re-fit on the training set, and predicted approval probabilities were generated for the test set. AUC was then calculated on these out-of-sample predictions.\n\nset.seed(42)\n\n# Split the data\ntrain_idx &lt;- sample(seq_len(nrow(exercise_data)), size = 0.8 * nrow(exercise_data))\ntrain_data &lt;- exercise_data[train_idx, ]\ntest_data  &lt;- exercise_data[-train_idx, ]\n\n# Refit model on training set\napproval_model &lt;- glm(\n  approved ~ income_500 + household_size + under18_n + over_59_n +\n    docs_with_app + docs_after_app + completion_time_capped +\n    stable_housing + interview_completed,\n  data = train_data,\n  family = binomial()\n)\n\n# Predict on test set\ntest_data &lt;- test_data |&gt;\n  mutate(predicted_prob = predict(approval_model, newdata = test_data, type = \"response\"))\n\n# AUC on test data\nroc_test &lt;- roc(test_data$approved, test_data$predicted_prob)\nauc(roc_test)\n\nArea under the curve: 0.7465\n\n\nInterpretation:\n\nAUC on the test set = 0.76, nearly identical to the in-sample AUC.\nThe model performs consistently on new data.\nThere is no sign of overfitting, and the results generalize well to similar applicants."
  },
  {
    "objectID": "analysis.html#predicted-probabilities",
    "href": "analysis.html#predicted-probabilities",
    "title": "Analysis Walkthrough",
    "section": "Predicted Probabilities",
    "text": "Predicted Probabilities\nThe logistic regression model produces a predicted probability of approval for each application. These values reflect how likely someone was to be approved, based on their reported information and process steps.\nLooking at predicted probabilities helps identify:\n\nWho was almost certain to be approved or denied\nWho was in a gray zone, where approval was uncertain\nWhere small changes — like completing an interview — might make a difference\n\n\nDistribution of Predicted Probabilities\nAdd predicted probabilities to the data:\n\nmodel_data &lt;- model.frame(approval_model) |&gt;\n  mutate(predicted_prob = predict(approval_model, type = \"response\"))\n\n\nggplot(model_data, aes(x = predicted_prob)) +\n  geom_histogram(fill = cfa_colors$blue, color = \"white\", bins = 30) +\n  labs(\n    title = \"Predicted Probability of CalFresh Approval\",\n    x = \"Predicted Probability\",\n    y = \"Number of Applicants\"\n  ) +\n  fnc_theme_cfa()\n\n\n\n\n\n\n\n\nInterpretation:\n\nMost applicants had predicted probabilities between 0.3 and 0.8, with two peaks:\n\nA major peak centered around 0.65\nA secondary peak around 0.8\n\nThere are fewer applicants with very low (near 0) or very high (near 1) probabilities, which makes sense — no single factor fully determines approval.\n\nThe distribution suggests real variability in approval likelihood — and that many applicants fall into a moderate range of uncertainty, not extremes.\n\n\nExample: Interview Completion\nTo show how much one variable matters, I compared average predicted probabilities by interview status:\n\nmodel_data |&gt;\n  group_by(interview_completed) |&gt;\n  summarize(\n    mean_pred_prob = round(mean(predicted_prob), 2),\n    n = n()\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    interview_completed = \"Interview Completed?\",\n    mean_pred_prob = \"Avg. Predicted Probability\",\n    n = \"N\"\n  ) |&gt;\n  gt::fmt_number(columns = n, decimals = 0) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\nInterview Completed?\nAvg. Predicted Probability\nN\n\n\n\n\nNot confirmed\n0.49\n1,161\n\n\nCompleted\n0.72\n472\n\n\n\n\n\n\n\nInterpretation:\n\nApplicants who completed the interview had a 72% average predicted chance of approval\nThose who did not (or did not confirm) had just a 50% chance\n\nThis 22-point gap is one of the clearest signals in the model — and points to a place where better support could help.\n\n\nSegmenting Applicants\nTo better understand who might benefit from support, I grouped applicants by predicted approval probability. This helps identify:\n\nWho is most likely to be approved or denied\nWho falls into a gray area, where outcomes are harder to predict\n\nDefine confidence bands:\n\nmodel_data &lt;- model_data |&gt;\n  mutate(prob_band = case_when(\n    predicted_prob &lt; 0.4 ~ \"Low (&lt;40%)\",\n    predicted_prob &gt;= 0.4 & predicted_prob &lt; 0.6 ~ \"Moderate (40–59%)\",\n    predicted_prob &gt;= 0.6 & predicted_prob &lt; 0.8 ~ \"High (60–79%)\",\n    predicted_prob &gt;= 0.8 ~ \"Very High (80%+)\"\n  ) |&gt; factor(levels = c(\"Low (&lt;40%)\", \"Moderate (40–59%)\", \"High (60–79%)\", \"Very High (80%+)\")))\n\nSummary by band:\n\nmodel_data |&gt;\n  group_by(prob_band) |&gt;\n  summarize(\n    n = n(),\n    actual_approval_rate = round(mean(approved, na.rm = TRUE) * 100, 1)\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    prob_band = \"Predicted Probability Band\",\n    n = \"N\",\n    actual_approval_rate = \"Observed Approval Rate (%)\"\n  ) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\nPredicted Probability Band\nN\nObserved Approval Rate (%)\n\n\n\n\nLow (&lt;40%)\n408\n23.0\n\n\nModerate (40–59%)\n316\n48.4\n\n\nHigh (60–79%)\n654\n67.9\n\n\nVery High (80%+)\n255\n85.9\n\n\n\n\n\n\n\nInterpretation:\n\nVery High (80%+): Most of these applicants were approved — minimal intervention needed.\nHigh (60–79%): Still strong performance, but some denials suggest small process gaps (e.g., missing docs).\nModerate (40–59%): This is the gray zone — almost half are denied. This group could benefit most from added support.\nLow (&lt;40%): Most were denied, but if any were income-eligible, this may indicate missed opportunities.\n\n\n\nGray Zone\nTo learn more about applicants in the 40–59% predicted range, I created a summary of their characteristics.\n\n# Get only the rows used in the model\nused_rows &lt;- as.numeric(rownames(model.frame(approval_model)))\n\n# Add predicted probabilities + other variables used in analysis\nmodel_data &lt;- exercise_data[used_rows, ] |&gt;\n  mutate(\n    predicted_prob = predict(approval_model, type = \"response\")\n  )\n\ngray_zone &lt;- model_data |&gt; \n  filter(predicted_prob &gt;= 0.4, predicted_prob &lt; 0.6)\n\ngray_zone_summary &lt;- gray_zone |&gt; \n  summarize(\n    n = n(),\n    approval_rate = round(mean(approved, na.rm = TRUE) * 100, 1),\n    pct_income_eligible = round(mean(income_eligible, na.rm = TRUE) * 100, 1),\n    pct_docs_with_app = round(mean(docs_with_app &gt; 0) * 100, 1),\n    pct_docs_after_app = round(mean(docs_after_app &gt; 0) * 100, 1),\n    pct_completed_interview = round(mean(interview_completed == \"Completed\") * 100, 1)\n  )\n\ngray_zone_summary |&gt;\n  pivot_longer(everything()) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    name = \"Metric\",\n    value = \"%\"\n  ) |&gt;\n  gt::fmt_number(columns = value, decimals = 1) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\nMetric\n%\n\n\n\n\nn\n316.0\n\n\napproval_rate\n50.6\n\n\npct_income_eligible\n98.4\n\n\npct_docs_with_app\n46.5\n\n\npct_docs_after_app\n24.4\n\n\npct_completed_interview\n31.3\n\n\n\n\n\n\n\nWhat stands out:\n\nNearly half of the gray zone applicants were approved\n99% appear income-eligible, but:\n\nOnly 38% submitted documents with their application\nOnly 22% completed the interview\n\n\nThis group represents a major opportunity: they’re likely eligible, but many didn’t complete the full process. Small nudges or reminders could meaningfully increase approvals."
  },
  {
    "objectID": "analysis.html#conclusion",
    "href": "analysis.html#conclusion",
    "title": "Analysis Walkthrough",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis examined patterns in CalFresh (SNAP) application outcomes among GetCalFresh.org users in San Diego County. Several process-related factors were strongly associated with whether an applicant was approved.\n\nKey Findings:\n\nInterview completion was the most predictive factor: Applicants who reported completing the interview were nearly three times more likely to be approved.\nDocument uploads mattered: Uploading verification documents — especially with the initial application — was associated with higher approval rates.\nHigher income reduced the odds of approval: This aligns with program rules. Each additional $500 in income lowered approval odds by about one-third.\nMany income-eligible applicants were not approved: Nearly half of income-eligible applicants were denied, suggesting process-related barriers (e.g., missing interviews or documents) play a major role.\nZIP code predicted approval differences: Approval rates varied significantly by ZIP, pointing to geographic disparities in access or processing.\nA large group fell into a “gray zone”: Applicants with a predicted approval probability between 40–59% were often income-eligible but lacked key follow-through steps. This is a high-impact group for outreach or nudges."
  },
  {
    "objectID": "analysis.html#next-steps-areas-for-deeper-analysis",
    "href": "analysis.html#next-steps-areas-for-deeper-analysis",
    "title": "Analysis Walkthrough",
    "section": "Next Steps: Areas for Deeper Analysis",
    "text": "Next Steps: Areas for Deeper Analysis\nThis section outlines follow-up analyses and design considerations to expand on the current findings and inform future improvements to the CalFresh application process.\n\nGeographic and Neighborhood Variation\n\nLink ZIP codes to American Community Survey (ACS) indicators:\n\nPoverty rate\nHousing burden\nBroadband access\nLanguages spoken at home\n\nMap approval rates by neighborhood to identify areas with potential access barriers\nAssess whether geographic disparities persist after controlling for applicant characteristics\n\n\n\nQualitative Research\n\nInterview applicants to:\n\nUnderstand perceived barriers in the process\nIdentify confusing or unclear steps\nExplore unmet needs for documentation or interview follow-up\n\nReview SMS or helpdesk interactions for common pain points\n\n\n\nTiming and Process Flow\n\nAnalyze time between:\n\nApplication start and finish\nApplication and document upload\nApplication and interview completion\n\nIdentify drop-off points or common delays in the flow\nExplore whether earlier intervention (e.g., reminders) affects outcomes\n\n\n\nAlternative Modeling Approaches\n\nTest interpretable models (e.g., decision trees) to identify interaction effects.\nUse cross-validation or bootstrap confidence intervals to strengthen robustness.\nExplore how assumptions about missing interviews affect results.\n\n\n\nApplicant Segmentation\n\nSegment applicants based on predicted probability and income eligibility\nIdentify profiles for targeted support:\n\nEligible but denied\nHigh predicted approval but not approved\n\n\n\n\nSystem and Policy Implications\n\nShare ZIP-level insights with county caseworkers and program administrators\nEvaluate platform changes (e.g., nudges, scheduling tools, help prompts) for impact on completion and approval\nExplore partnerships for assisted application support in low-approval ZIPs"
  }
]