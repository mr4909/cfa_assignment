[
  {
    "objectID": "key_findings.html",
    "href": "key_findings.html",
    "title": "Key Findings",
    "section": "",
    "text": "This analysis draws on 2,046 CalFresh applications submitted online in San Diego County. It includes applicant information, activity on the site, and final approval outcomes — but does not capture actions taken outside the platform, like mailed documents or phone interviews."
  },
  {
    "objectID": "key_findings.html#factors-associated-with-approval",
    "href": "key_findings.html#factors-associated-with-approval",
    "title": "Key Findings",
    "section": "1. Factors Associated With Approval",
    "text": "1. Factors Associated With Approval\nTo understand what predicts CalFresh approval, I fit a logistic regression model using application data from GetCalFresh.org. The model estimates how each factor — like income, document uploads, and interview completion — is associated with the chance of being approved, while accounting for all other variables in the dataset.\nVariables were selected based on program relevance, user behavior, and insights from exploratory analysis. I scaled income per $500 for interpretability and recoded interview completion to retain applicants with missing responses.\nThe goal wasn’t just to predict outcomes, but to understand where applicants might drop off and which steps in the process matter most.\n\nKey Findings:\n\nApplicants who confirmed completing the interview had a predicted approval rate of 72%, compared to 50% for those who didn’t confirm.\nUploading documents with the application increased approval chances.\nHigher income reduced the likelihood of approval — even among mostly income-eligible applicants.\nHaving children in the household was associated with higher approval odds.\nHousing stability and application time didn’t show strong associations once other factors were considered.\n\n\nThe table below shows model results using odds ratios — a way to estimate how each factor affects the odds of approval, controlling for all others. For example, an odds ratio of 1.5 means the odds of approval are 50% higher for that group compared to the baseline.\n\nThe model explained a meaningful amount of variation in approval outcomes (McFadden R² = 0.15) and correctly distinguished approved vs. denied applications 76% of the time (AUC = 0.76). These results suggest the model performs well given the limited behavioral data available from the application platform.\n\nTo make the results easier to interpret, the table below shows predicted approval rates for example applicant scenarios, based on the fitted model.\nThese results show that relatively small steps — like completing an interview or uploading a document — can meaningfully increase the chance of approval. Many of these steps could be supported through timely nudges, simpler workflows, or user-centered reminders."
  },
  {
    "objectID": "key_findings.html#potential-improvements",
    "href": "key_findings.html#potential-improvements",
    "title": "Key Findings",
    "section": "2. Potential Improvements",
    "text": "2. Potential Improvements\nThe model points to clear opportunities to improve approval outcomes by supporting applicants at key decision points.\n\nRecommendations:\n\nSupport interview completion: Many eligible applicants do not confirm completing the interview. Providing reminders, real-time scheduling, or alternative follow-up methods could increase follow-through.\nEncourage early document uploads: Uploading documents with the application was strongly associated with approval. Nudging users to upload early — especially those likely to qualify — could reduce denials.\nAddress geographic disparities: Approval rates vary significantly by ZIP code. Further analysis could explore whether these reflect staffing, broadband access, or population needs — and help inform place-based outreach strategies.\n\n\nStrengthening these steps would not only increase approval rates, but also improve equity and access for those most in need."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "For questions or additional information, feel free to contact:\nMari Roberts\nProject Author\nEmail: marialexandriaroberts@gmail.com\nGitHub: @mr4909"
  },
  {
    "objectID": "contact.html#team",
    "href": "contact.html#team",
    "title": "Contact",
    "section": "",
    "text": "For questions or additional information, feel free to contact:\nMari Roberts\nProject Author\nEmail: marialexandriaroberts@gmail.com\nGitHub: @mr4909"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This analysis was conducted as part of a take-home exercise for Code for America.\nIt explores which factors are associated with approval of CalFresh (SNAP) applications submitted via GetCalFresh.org in San Diego County.\nThe goal is to identify patterns in approval outcomes and highlight steps that could help more eligible applicants access food assistance."
  },
  {
    "objectID": "about.html#objectives",
    "href": "about.html#objectives",
    "title": "About",
    "section": "Objectives",
    "text": "Objectives\n\nIdentify which application characteristics are most strongly associated with approval.\nUnderstand where the process may break down (e.g., interviews, document submission).\nSuggest actionable, user-centered improvements to reduce friction in the process.\n\n\nKey Questions\n\nWhat factors are most strongly associated with CalFresh approval?\nWhere would you look next to improve the application process?"
  },
  {
    "objectID": "about.html#project-links",
    "href": "about.html#project-links",
    "title": "About",
    "section": "Project Links",
    "text": "Project Links\n\nGitHub Repository\nExercise Prompt\nDownload from Google Drive"
  },
  {
    "objectID": "about.html#stakeholders-considered",
    "href": "about.html#stakeholders-considered",
    "title": "About",
    "section": "Stakeholders Considered",
    "text": "Stakeholders Considered\n\nGetCalFresh Team – program design and outreach\n\nCounty Human Services – eligibility decision-makers\n\nApplicants – especially those facing barriers to access"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis Walkthrough",
    "section": "",
    "text": "This section documents the full analysis process—from data preparation to modeling to interpretation.\nFor a summary of insights, visit the Key Findings page.\nFor project context, visit the About page."
  },
  {
    "objectID": "analysis.html#overview",
    "href": "analysis.html#overview",
    "title": "Analysis Walkthrough",
    "section": "",
    "text": "This section documents the full analysis process—from data preparation to modeling to interpretation.\nFor a summary of insights, visit the Key Findings page.\nFor project context, visit the About page."
  },
  {
    "objectID": "analysis.html#application-walkthrough",
    "href": "analysis.html#application-walkthrough",
    "title": "Analysis Walkthrough",
    "section": "Application Walkthrough",
    "text": "Application Walkthrough\nBefore analyzing the data, I walked through the GetCalFresh.org application process myself. This helped clarify how each field is presented to applicants, which steps are required or optional, and where users might drop-off.\n\nKey Takeaways\n\nMultilingual support is available early in the application (English, Spanish, Chinese, Vietnamese), with additional language preferences captured later.\nThe application is structured in stages: household info → income → expenses → contact details → confirmation.\nApplicants get real-time feedback about possible eligibility or ineligibility.\nDocument uploads and interviews aren’t required at submission — they can happen later, which means missing data doesn’t always signal ineligibility.\nToward the end, applicants confirm contact information, set preferences (e.g., language, reminders), and indicate interview availability.\n\nThis walkthrough was useful for interpreting behavioral data in context — especially steps that are optional or invisible in the dataset (like phone interviews completed outside the platform)."
  },
  {
    "objectID": "analysis.html#about-the-data",
    "href": "analysis.html#about-the-data",
    "title": "Analysis Walkthrough",
    "section": "About the Data",
    "text": "About the Data\nThis dataset includes 2,046 CalFresh (SNAP) applications submitted through GetCalFresh.org in San Diego County. Each row represents one application and contains:\n\nInformation reported by the applicant\nActivity tracked through the GetCalFresh platform\nFinal approval outcome provided by the county\n\nThe dataset reflects the user-facing side of the process. It does not capture every factor a county worker might see (e.g., paper documents or offline interviews), but it provides a detailed view of what users did on the site and what happened afterward.\nKey Variables\n\n\n\n\n\n\n\n\nVariable\nDescription\nNotes\n\n\n\n\nincome\nHousehold income in the last 30 days\nSlightly randomized for privacy\n\n\nhousehold_size\nNumber of people applying\nUsed to determine income thresholds\n\n\ndocs_with_app\nDocuments uploaded with the application\nOptional at time of submission\n\n\ndocs_after_app\nDocuments uploaded after submission\nOften submitted after the interview\n\n\nhad_interview\nApplicant’s self-report of completing the interview\nBased on SMS follow-up; may be missing\n\n\ncompletion_time_mins\nTime taken to complete the application\nMay include pauses or returns\n\n\nstable_housing\nWhether applicant rents/owns their sleeping location\nProxy for housing stability\n\n\nunder18_n, over_59_n\nChildren or older adults in the household\nMay influence prioritization or eligibility\n\n\nzip\nApplicant ZIP code\nMay reflect local conditions or access issues\n\n\napproved\nFinal approval outcome\nProvided by the county\n\n\n\nContextual Notes:\n\nInterview completion (had_interview) comes from an SMS response. Missing values do not confirm whether an interview occurred — only that no reply was recorded.\nDocument fields only include uploads through GetCalFresh. Submissions made by mail, fax, or in person are not tracked here.\nApproval decisions come from county records and represent real outcomes.\n\nUnderstanding what’s captured — and what’s missing — helped guide how I interpreted variables."
  },
  {
    "objectID": "analysis.html#exploratory-data-analysis",
    "href": "analysis.html#exploratory-data-analysis",
    "title": "Analysis Walkthrough",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore modeling, I conducted an exploratory analysis to:\n\nUnderstand the distribution and structure of each variable\nIdentify missingness and data quality issues\nSpot early signals related to approval\nPrepare features for interpretability and modeling\n\n\nCodebook Summary\nI created a structured codebook using a function from my own databookR package. It describes each variable, its type, missingness, and key statistics — and forms the foundation for all further steps.\n\n\nCode\n# Generate codebook\ncodebook &lt;- databookR::databook(exercise_data)\n\n# Style codebook\nfnc_style_reactable_table(\n  data = codebook |&gt; dplyr::select(`Variable Name`, Statistics, dplyr::everything()),\n  columns = list(\n    `Variable Name` = colDef(align = \"left\", width = 200),\n    Statistics = colDef(align = \"left\", html = TRUE, width = 200)\n  )\n)\n\n\n\n\n\n\nI next reviewed distributions of numeric variables to check for skew, outliers, and interpretability issues.\n\n\nCode\n# Custom binwidths depending on variable\nbinwidths &lt;- list(\n  income = 250,\n  completion_time_mins = 40,\n  docs_with_app = 1,\n  docs_after_app = 1,\n  household_size = 1,\n  under18_n = 1,\n  over_59_n = 1\n)\n\n# Generate all plots with custom function\nwrap_plots(\n  fnc_plot_var(\"income\", \"Monthly Income\"),\n  fnc_plot_var(\"completion_time_mins\", \"App Completion Time (Minutes)\", max_x = 120),\n  fnc_plot_var(\"docs_with_app\", \"Docs Uploaded With App\"),\n  fnc_plot_var(\"docs_after_app\", \"Docs Uploaded After App\"),\n  fnc_plot_var(\"household_size\", \"Household Size\"),\n  fnc_plot_var(\"under18_n\", \"Children in Household\"),\n  fnc_plot_var(\"over_59_n\", \"Older Adults in Household\"),\n  ncol = 2\n)\n\n\n\n\n\n\n\n\n\nObservations:\n\nIncome is heavily right-skewed; most applicants report $0 or very low income (median = $270/month).\nApplication time is short for most (median = 10 minutes), but some outliers take hours.\nDocument uploads are rare — the majority submit no documents online, either before or after applying.\nHousehold composition: Most applicants apply alone or with one other person. Few list dependents under 18 or over 59.\nStable housing is reported by only 58% — suggesting nearly half of applicants face housing instability.\nApproval rate = 56%, with enough variation for modeling.\nInterview data is missing for ~50% of cases — expected, since it’s collected via optional follow-up SMS and not all applicants respond.\nAll other variables are fully or nearly complete.\n\nThis overview helped identify missing or unusual values, clarify behavioral patterns, and flag variables likely to influence approval.\n\n\nCorrelation and Redundancy Check\nBefore modeling, I examined relationships among predictors to identify potential multicollinearity or redundancy. This ensures that coefficients remain stable and interpretable.\nMethod 1: Correlation Matrix\nI computed pairwise correlations among numeric variables.\n\n\nCode\n# Select numeric predictors\nnum_vars &lt;- exercise_data |&gt;\n  select(income, household_size, under18_n, over_59_n, \n         docs_with_app, docs_after_app, completion_time_mins)\n\n# Compute correlations\ncor_matrix &lt;- cor(num_vars, use = \"complete.obs\")\n\n# Visualize\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         tl.cex = 2, tl.col = \"black\",\n         col = colorRampPalette(c(cfa_colors$red, \"white\", cfa_colors$blue))(200))\n\n\n\n\n\n\n\n\n\nObservations:\n\nunder18_n and household_size are strongly correlated (r = 0.89), which is expected — families with children are generally larger.\nincome and household_size are moderately correlated but not collinear in a way that impairs model performance.\n\nMethod 2: Variance Inflation Factor (VIF)\nA Variance Inflation Factor (VIF) measures how much a variable’s estimated coefficient is inflated due to correlation with other predictors — higher values suggest multicollinearity.\n\n\nCode\n# Quick VIF check with basic logistic model\nvif_model &lt;- glm(\n  approved ~ income + household_size + under18_n + over_59_n +\n    docs_with_app + docs_after_app + completion_time_mins + \n    stable_housing + had_interview,\n  data = exercise_data,\n  family = binomial()\n)\n\ncar::vif(vif_model)\n\n\n              income       household_size            under18_n \n            1.570242             4.949500             4.810768 \n           over_59_n        docs_with_app       docs_after_app \n            1.082801             1.071075             1.094315 \ncompletion_time_mins       stable_housing        had_interview \n            1.012481             1.241863             1.105751 \n\n\nObservations:\n\nhousehold_size (4.95) and under18_n (4.81) are near the upper limit but acceptable.\nincome (1.57) shows no collinearity concern.\nAll other variables have low VIFs (&lt;1.3), indicating no issues.\nNo variables exceed the common threshold of 5 — multicollinearity is not a concern.\n\nBoth variables household_size and under18_n, will likely be retained. While correlated, they reflect different eligibility factors: household size affects income limits, while the presence of children may affect processing or priority."
  },
  {
    "objectID": "analysis.html#approval-rates-by-key-variables",
    "href": "analysis.html#approval-rates-by-key-variables",
    "title": "Analysis Walkthrough",
    "section": "Approval Rates by Key Variables",
    "text": "Approval Rates by Key Variables\nTo understand where in the process outcomes start to diverge, I looked at approval rates across key variables. This helped identify patterns worth modeling and showed possible intervention points.\n\nInterview Completion\n\n\nCode\nfnc_approval_summary(exercise_data, had_interview)\n\n\n\n\n\n\n\n\nGroup\nN\nApproval Rate (%)\n\n\n\n\nFALSE\n434\n50.0\n\n\nTRUE\n595\n72.1\n\n\nNA\n1,017\n49.4\n\n\n\n\n\n\n\nObservations:\n\nApplicants who reported completing the interview had higher approval rates than those who did not or whose response was missing.\nThis reinforces the interview as a critical point of potential drop-off.\nMissing responses likely indicate no follow-up engagement — not necessarily ineligibility.\n\n\n\nDocument Submission Group\nI grouped applicants by when (or whether) they submitted documents.\n\n\nCode\n# Add a doc_group variable \nexercise_data &lt;- exercise_data |&gt; \n  mutate(\n    doc_group = case_when(\n        docs_with_app &gt; 0 & docs_after_app == 0 ~ \"With App Only\",\n        docs_with_app == 0 & docs_after_app &gt; 0 ~ \"After App Only\",\n        docs_with_app &gt; 0 & docs_after_app &gt; 0 ~ \"With + After\",\n        docs_with_app == 0 & docs_after_app == 0 ~ \"No Docs\"\n        ) |&gt; \n      factor(levels = c(\"With App Only\", \"After App Only\", \"With + After\", \"No Docs\"))\n      )\nfnc_approval_summary(exercise_data, doc_group)\n\n\n\n\n\n\n\n\nGroup\nN\nApproval Rate (%)\n\n\n\n\nWith App Only\n797\n65.0\n\n\nAfter App Only\n248\n57.3\n\n\nWith + After\n213\n65.7\n\n\nNo Docs\n788\n44.2\n\n\n\n\n\n\n\nObservations:\n\nApproval was highest among applicants who submitted documents with their initial application.\nApplicants who submitted documents only after applying had moderately lower rates.\nThe lowest approval rate (~44%) was among those who submitted nothing online.\n\nThis suggests that early document submission may signal follow-through — or speed up processing.\n\n\nHousing Stability\n\n\nCode\nfnc_approval_summary(exercise_data, stable_housing)\n\n\n\n\n\n\n\n\nGroup\nN\nApproval Rate (%)\n\n\n\n\nFALSE\n860\n64.4\n\n\nTRUE\n1,186\n50.1\n\n\n\n\n\n\n\nObservations:\n\nApplicants reporting unstable housing had higher approval rates.\nThis may reflect prioritized eligibility for those experiencing housing instability.\nHousing instability may increase likelihood of approval due to expedited or simplified eligibility pathways.\n\n\n\nHousehold Size (Binned)\nI binned household size for interpretability.\n\n\nCode\n# Add bins\nexercise_data &lt;- exercise_data |&gt; \n  mutate(household_size_bin = cut(household_size, breaks = c(0,1,2,3,5,10)))\n\nfnc_approval_summary(exercise_data, household_size_bin)\n\n\n\n\n\n\n\n\nGroup\nN\nApproval Rate (%)\n\n\n\n\n(0,1]\n1,223\n61.7\n\n\n(1,2]\n334\n52.1\n\n\n(2,3]\n227\n45.4\n\n\n(3,5]\n228\n46.1\n\n\n(5,10]\n29\n27.6\n\n\nNA\n5\n60.0\n\n\n\n\n\n\n\nObservations:\n\n1–2 person households had the highest approval rates.\nLarger households saw lower approval rates.\nThis may be due to stricter income thresholds at higher household sizes — or more complexity in verifying eligibility.\n\n\n\nZIP Code Variation\nZIP code can reflect structural factors that influence access: geography, internet connectivity, support, and even worker caseloads. While it’s not causal, it helps show system-level variation.\nI assessed variation in approval rates by ZIP code, filtering out ZIPs with fewer than 10 applications.\n\n\nCode\n# Aggregate by ZIP (filter out sparse ZIPs)\nzip_summary &lt;- exercise_data |&gt;\n  group_by(zip) |&gt;\n  summarize(\n    n = n(),\n    approval_rate = mean(approved, na.rm = TRUE)\n  ) |&gt;\n  filter(n &gt;= 10)\n\n# Bar chart\nggplot(zip_summary, aes(x = fct_reorder(zip, approval_rate), \n                        y = approval_rate * 100)) +\n  geom_col(fill = cfa_colors$blue) +\n  coord_flip() +\n  labs(\n    title = \"Approval Rate by ZIP Code (≥10 applications)\",\n    x = \"ZIP Code\",\n    y = \"Approval Rate (%)\"\n  ) +\n  fnc_theme_cfa()\n\n\n\n\n\n\n\n\n\nObservations:\n\nApproval rates vary from ~34% to ~69% across ZIP codes.\nThis range is large enough to suggest systematic differences, not just noise.\nHigh- and low-performing ZIPs each have reasonable sample sizes, supporting this concern.\n\n\n\nStatistical Test: Is ZIP Predictive of Approval?\nTo formally test whether ZIP code is predictive of approval, I ran a chi-squared test.\n\n\nCode\nzip_test &lt;- exercise_data |&gt;\n  filter(!is.na(approved), !is.na(zip)) |&gt;\n  count(zip, approved) |&gt;\n  pivot_wider(names_from = approved, values_from = n, values_fill = 0) |&gt;\n  column_to_rownames(\"zip\") |&gt;\n  as.matrix() |&gt;\n  chisq.test()\n\nzip_test\n\n\n\n    Pearson's Chi-squared test\n\ndata:  as.matrix(column_to_rownames(pivot_wider(count(filter(exercise_data,     !is.na(approved), !is.na(zip)), zip, approved), names_from = approved,     values_from = n, values_fill = 0), \"zip\"))\nX-squared = 43.85, df = 27, p-value = 0.02143\n\n\nObservations:\n\nThe chi-squared test was statistically significant (p &lt; 0.05).\nThis means approval rates vary by ZIP code more than we’d expect by chance.\n\n\n\nPreparation for Modeling\nBefore fitting a model, I created new variables and adjusted a few existing ones to improve interpretability. These changes were grounded in earlier exploratory analysis and CalFresh eligibility rules.\nThe goal was to make model coefficients easier to interpret and ensure alignment with how eligibility and case processing work in practice.\nIncome\n\n\nCode\nexercise_data &lt;- exercise_data |&gt;\n  mutate(income_500 = income / 500)\n\n\n\nScaled income in $500 units to make coefficients easier to interpret.\nA log-odds change now reflects the effect of each additional ~$500 in monthly income, not each dollar.\n\nApplication Completion Time\n\n\nCode\nexercise_data &lt;- exercise_data |&gt;\n  mutate(\n    completion_time_capped = if_else(completion_time_mins &gt; 180, 180, completion_time_mins),\n    long_app = completion_time_mins &gt; 60\n  )\n\n\n\nCapped extreme values above 180 minutes to reduce the influence outliers.\nFlagged apps that took more than one hour (long_app) to explore whether interruptions or complexity affect outcomes.\n\nInterview Completion (Self-Reported)\n\n\nCode\nexercise_data &lt;- exercise_data |&gt;\n  mutate(interview_completed = case_when(\n    had_interview == TRUE ~ \"Completed\",\n    TRUE ~ \"Not confirmed\"\n  ) |&gt; factor(levels = c(\"Not confirmed\", \"Completed\")))\n\n\n\nRe-coded had_interview into a 2-category factor:\n\n“Completed” = applicant said they had the interview\n“Not confirmed” = didn’t respond or said no\n\nThis avoids misinterpreting missing data as a definitive “no”"
  },
  {
    "objectID": "analysis.html#income-eligibility",
    "href": "analysis.html#income-eligibility",
    "title": "Analysis Walkthrough",
    "section": "Income Eligibility",
    "text": "Income Eligibility\nTo better understand who should be approved under CalFresh rules, I used the official income eligibility thresholds based on household size.\nThese limits reflect the 200% Federal Poverty Level under California’s Broad-Based Categorical Eligibility (BBCE) policy.\nThis allows us to distinguish:\n\nApplicants who likely met income-based eligibility\nApplicants who may have been denied despite being income-eligible\nThe extent to which approval decisions align with income thresholds\n\n\n\nCode\n# Create eligibility table based on the table here:\n# https://dpss.lacounty.gov/en/food/calfresh/gross-income.html\neligibility_table &lt;- tibble::tibble(\n  household_size = 1:15,\n  max_gross_income = c(\n    2510, 3408, 4304, 5200, 6098, 6994, 7890, 8788,\n    9686, 10582, 11478, 12374, 13270, 14166, 15062  # estimate using +896 per person\n  )\n)\n\n# Join with application data\nexercise_data &lt;- exercise_data |&gt;\n  left_join(eligibility_table, by = \"household_size\") |&gt;\n  mutate(\n    income_eligible = income &lt;= max_gross_income\n  )\n\n# Approval summary by income eligibility\napproval_summary &lt;- exercise_data |&gt;\n  group_by(income_eligible) |&gt;\n  summarize(\n    n = n(),\n    approval_rate = mean(approved, na.rm = TRUE),\n    approved_over_income = sum(approved & !income_eligible, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    approval_rate = round(approval_rate * 100, 1)\n  )\n\n# Highlight minimum approval rate\nmin_rate &lt;- min(approval_summary$approval_rate, na.rm = TRUE)\n\napproval_summary |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    income_eligible = \"Income-Eligible\",\n    n = \"N\",\n    approval_rate = \"Approval Rate (%)\",\n    approved_over_income = \"Approved Despite High Income\"\n  ) |&gt;\n  gt::fmt_number(columns = c(n, approved_over_income), decimals = 0) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\n\nIncome-Eligible\nN\nApproval Rate (%)\nApproved Despite High Income\n\n\n\n\nFALSE\n43\n9.3\n4\n\n\nTRUE\n1,999\n57.1\n0\n\n\nNA\n4\n50.0\n0\n\n\n\n\n\n\n\nObservations:\n\nMost applicants appear income-eligible.\nA small number were approved despite exceeding income thresholds — potentially due to exceptions, data entry errors, or income verification.\nNearly half of income-eligible applicants were not approved, pointing to process barriers like missed interviews or incomplete documentation.\n\nThis flag helps contextualize approval decisions in the model, especially when eligible applicants are denied."
  },
  {
    "objectID": "analysis.html#logistic-regression",
    "href": "analysis.html#logistic-regression",
    "title": "Analysis Walkthrough",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nTo identify factors most strongly associated with CalFresh approval, I fit a logistic regression model. This model estimates the likelihood of approval based on eligibility-related variables and applicant actions observed through the application process.\n\nVariable Selection Rationale\nI included variables based on:\n\nProgram relevance (e.g., income, household size)\nUser experience (e.g., document upload, interview)\nResults from earlier exploratory analysis\n\nThe final model uses:\n\nScaled income: income_500\nHousehold composition: household_size, under18_n, over_59_n\nDocument submission: docs_with_app, docs_after_app\nTime spent applying: completion_time_capped\nHousing stability: stable_housing\nInterview completion: interview_completed (re-coded)\n\n\n\nModel Specification\n\n\nCode\napproval_model &lt;- glm(\n  approved ~ income_500 + household_size + under18_n + over_59_n +\n    docs_with_app + docs_after_app + completion_time_capped +\n    stable_housing + interview_completed,\n  data = exercise_data,\n  family = binomial()\n)\n\nsummary(approval_model)\n\n\n\nCall:\nglm(formula = approved ~ income_500 + household_size + under18_n + \n    over_59_n + docs_with_app + docs_after_app + completion_time_capped + \n    stable_housing + interview_completed, family = binomial(), \n    data = exercise_data)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   0.628432   0.131272   4.787 1.69e-06 ***\nincome_500                   -0.412618   0.029816 -13.839  &lt; 2e-16 ***\nhousehold_size               -0.125768   0.088893  -1.415  0.15712    \nunder18_n                     0.300545   0.117048   2.568  0.01024 *  \nover_59_n                     0.121020   0.155961   0.776  0.43777    \ndocs_with_app                 0.116576   0.025326   4.603 4.16e-06 ***\ndocs_after_app                0.075872   0.027076   2.802  0.00507 ** \ncompletion_time_capped       -0.001958   0.002632  -0.744  0.45674    \nstable_housingTRUE           -0.086942   0.112313  -0.774  0.43887    \ninterview_completedCompleted  1.098309   0.118276   9.286  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2800.1  on 2041  degrees of freedom\nResidual deviance: 2373.5  on 2032  degrees of freedom\n  (4 observations deleted due to missingness)\nAIC: 2393.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nObservations:\n\nInterview completed had the strongest association with approval.\nHigher income reduced the odds of approval, as expected.\nDocument uploads (both with and after application) were positively associated with approval.\nEach additional child was associated with higher approval odds.\nOther variables (e.g., housing stability, household size, older adults, app time) were not significant once the above were accounted for.\n\n\n\nOdds Ratios and Confidence Intervals\n\n\nCode\nmodel_results &lt;- broom::tidy(approval_model, exponentiate = TRUE, conf.int = TRUE)\n\nmodel_results |&gt;\n  select(term, estimate, conf.low, conf.high, p.value) |&gt;\n  mutate(across(where(is.numeric), round, 2)) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    term = \"Variable\",\n    estimate = \"Odds Ratio\",\n    conf.low = \"95% CI (Low)\",\n    conf.high = \"95% CI (High)\",\n    p.value = \"P-Value\"\n  ) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\n\nVariable\nOdds Ratio\n95% CI (Low)\n95% CI (High)\nP-Value\n\n\n\n\n(Intercept)\n1.87\n1.45\n2.43\n0.00\n\n\nincome_500\n0.66\n0.62\n0.70\n0.00\n\n\nhousehold_size\n0.88\n0.74\n1.05\n0.16\n\n\nunder18_n\n1.35\n1.07\n1.70\n0.01\n\n\nover_59_n\n1.13\n0.83\n1.53\n0.44\n\n\ndocs_with_app\n1.12\n1.07\n1.18\n0.00\n\n\ndocs_after_app\n1.08\n1.02\n1.14\n0.01\n\n\ncompletion_time_capped\n1.00\n0.99\n1.00\n0.46\n\n\nstable_housingTRUE\n0.92\n0.74\n1.14\n0.44\n\n\ninterview_completedCompleted\n3.00\n2.38\n3.79\n0.00\n\n\n\n\n\n\n\nObservations:\n\nAn odds ratio &gt; 1 means the variable is associated with a higher chance of approval\nAn odds ratio &lt; 1 means a lower chance\nInterview completed: ~3x higher odds of approval\nEach $500 in income: ~34% lower odds\nEach document uploaded with the application: ~12% higher odds\nEach child (under 18): ~35% higher odds\n\nThese results reinforce earlier descriptive findings — but also show that certain variables (like app duration or housing status) have little added explanatory value once core factors are controlled for."
  },
  {
    "objectID": "analysis.html#diagnostics",
    "href": "analysis.html#diagnostics",
    "title": "Analysis Walkthrough",
    "section": "Diagnostics",
    "text": "Diagnostics\nAfter fitting the logistic regression, I ran several checks to evaluate how well the model fits the data and whether the results are trustworthy. These diagnostics focus on:\n\nHow much variation the model explains\nWhether predicted probabilities align with actual outcomes\nHow well the model distinguishes approved vs. denied applications\n\n1. McFadden’s Psuedo R²\nDefinition: A measure of how much better the model fits the data compared to a model with no predictors (just an intercept).\n\n\nCode\n# Pseudo R-squared\npscl::pR2(approval_model)\n\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-1186.7634616 -1400.0644571   426.6019909     0.1523508     0.1885348 \n         r2CU \n    0.2526548 \n\n\nThe pseudo R² was around 0.15, which indicates a moderate effect size. That’s typical in behavioral data, where many influencing factors aren’t captured in the dataset.\n2. Hosmer–Lemeshow Goodness-of-Fit Test\nThis test checks whether the model’s predicted probabilities align with the actual outcomes. It groups observations into deciles by predicted probability, then compares predicted vs. actual approval rates in each group.\n\n\nCode\nhoslem.test(approval_model$y, fitted(approval_model))\n\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  approval_model$y, fitted(approval_model)\nX-squared = 9.0452, df = 8, p-value = 0.3385\n\n\nOur p-value is 0.34, which is not staistically significant. That’s good — it means there’s no evidence of poor fit. The model’s predictions are consistent with observed data.\n3. ROC Curve and AUC (Area Under the Curve)\nAUC summarizes how well the model distinguishes between approved and denied applicants.\n\n\nCode\nmodel_data &lt;- model.frame(approval_model)\nactual &lt;- model_data$approved\npredicted &lt;- fitted(approval_model)\n\nroc_obj &lt;- roc(actual, predicted)\nplot(roc_obj, col = cfa_colors$blue, lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\npROC::auc(roc_obj)\n\n\nArea under the curve: 0.7563\n\n\nObservations:\n\nAUC = 0.76 means the model assigns a higher predicted probability to an approved case than a denied one 76% of the time.\nThat’s considered good performance for a logistic model using only observable application behaviors.\n\nTogether, these diagnostics show that the model is well-calibrated, explains meaningful variation, and performs reliably — even with known limitations in the dataset.\n4. Train/Test Split\nTo evaluate how well the model generalizes, I randomly split the data into:\n\n80% training set (used to fit the model)\n20% test set (used to evaluate performance on unseen data)\n\nThe model was re-fit on the training set, and predicted approval probabilities were generated for the test set. AUC was then calculated on these out-of-sample predictions.\n\n\nCode\nset.seed(42)\n\n# Split the data\ntrain_idx &lt;- sample(seq_len(nrow(exercise_data)), size = 0.8 * nrow(exercise_data))\ntrain_data &lt;- exercise_data[train_idx, ]\ntest_data  &lt;- exercise_data[-train_idx, ]\n\n# Refit model on training set\napproval_model &lt;- glm(\n  approved ~ income_500 + household_size + under18_n + over_59_n +\n    docs_with_app + docs_after_app + completion_time_capped +\n    stable_housing + interview_completed,\n  data = train_data,\n  family = binomial()\n)\n\n# Predict on test set\ntest_data &lt;- test_data |&gt;\n  mutate(predicted_prob = predict(approval_model, newdata = test_data, type = \"response\"))\n\n# AUC on test data\nroc_test &lt;- roc(test_data$approved, test_data$predicted_prob)\nauc(roc_test)\n\n\nArea under the curve: 0.7465\n\n\nObservations:\n\nAUC on the test set = 0.76, nearly identical to the in-sample AUC.\nThe model performs consistently on new data.\nThere is no sign of overfitting, and the results generalize well to similar applicants."
  },
  {
    "objectID": "analysis.html#predicted-probabilities",
    "href": "analysis.html#predicted-probabilities",
    "title": "Analysis Walkthrough",
    "section": "Predicted Probabilities",
    "text": "Predicted Probabilities\nThe logistic regression model produces a predicted probability of approval for each application. These values reflect how likely someone was to be approved, based on their reported information and process steps.\nLooking at predicted probabilities helps identify:\n\nWho was almost certain to be approved or denied\nWho was in a gray zone, where approval was uncertain\nWhere small changes — like completing an interview — might make a difference\n\n\nDistribution of Predicted Probabilities\n\n\nCode\nmodel_data &lt;- model.frame(approval_model) |&gt;\n  mutate(predicted_prob = predict(approval_model, type = \"response\"))\n\n\n\n\nCode\nggplot(model_data, aes(x = predicted_prob)) +\n  geom_histogram(fill = cfa_colors$blue, color = \"white\", bins = 30) +\n  labs(\n    title = \"Predicted Probability of CalFresh Approval\",\n    x = \"Predicted Probability\",\n    y = \"Number of Applicants\"\n  ) +\n  fnc_theme_cfa()\n\n\n\n\n\n\n\n\n\nObservations:\n\nMost applicants had predicted probabilities between 0.3 and 0.8, with two peaks:\n\nA major peak centered around 0.65\nA secondary peak around 0.8\n\nThere are fewer applicants with very low (near 0) or very high (near 1) probabilities, which makes sense — no single factor fully determines approval.\n\nThe distribution suggests real variability in approval likelihood — and that many applicants fall into a moderate range of uncertainty, not extremes.\n\n\nExample: Interview Completion\nTo show how much one variable matters, I compared average predicted probabilities by interview status:\n\n\nCode\nmodel_data |&gt;\n  group_by(interview_completed) |&gt;\n  summarize(\n    mean_pred_prob = round(mean(predicted_prob), 2),\n    n = n()\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    interview_completed = \"Interview Completed\",\n    mean_pred_prob = \"Avg. Predicted Probability\",\n    n = \"N\"\n  ) |&gt;\n  gt::fmt_number(columns = n, decimals = 0) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\n\nInterview Completed\nAvg. Predicted Probability\nN\n\n\n\n\nNot confirmed\n0.49\n1,161\n\n\nCompleted\n0.72\n472\n\n\n\n\n\n\n\nObservations:\n\nApplicants who completed the interview had a 72% average predicted chance of approval\nThose who did not (or did not confirm) had just a 50% chance\n\nThis 22-point gap is one of the clearest signals in the model — and points to a place where better support could help.\n\n\nSegmenting Applicants\nTo better understand who might benefit from support, I grouped applicants by predicted approval probability. This helps identify:\n\nWho is most likely to be approved or denied\nWho falls into a gray area, where outcomes are harder to predict\n\nDefine confidence bands:\n\n\nCode\nmodel_data &lt;- model_data |&gt;\n  mutate(prob_band = case_when(\n    predicted_prob &lt; 0.4 ~ \"Low (&lt;40%)\",\n    predicted_prob &gt;= 0.4 & predicted_prob &lt; 0.6 ~ \"Moderate (40–59%)\",\n    predicted_prob &gt;= 0.6 & predicted_prob &lt; 0.8 ~ \"High (60–79%)\",\n    predicted_prob &gt;= 0.8 ~ \"Very High (80%+)\"\n  ) |&gt; factor(levels = c(\"Low (&lt;40%)\", \"Moderate (40–59%)\", \"High (60–79%)\", \"Very High (80%+)\")))\n\n\nSummary by band:\n\n\nCode\nmodel_data |&gt;\n  group_by(prob_band) |&gt;\n  summarize(\n    n = n(),\n    actual_approval_rate = round(mean(approved, na.rm = TRUE) * 100, 1)\n  ) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    prob_band = \"Predicted Probability Band\",\n    n = \"N\",\n    actual_approval_rate = \"Observed Approval Rate (%)\"\n  ) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\n\nPredicted Probability Band\nN\nObserved Approval Rate (%)\n\n\n\n\nLow (&lt;40%)\n408\n23.0\n\n\nModerate (40–59%)\n316\n48.4\n\n\nHigh (60–79%)\n654\n67.9\n\n\nVery High (80%+)\n255\n85.9\n\n\n\n\n\n\n\nObservations:\n\nVery High (80%+): Most of these applicants were approved — minimal intervention needed.\nHigh (60–79%): Still strong performance, but some denials suggest small process gaps (e.g., missing docs).\nModerate (40–59%): This is the gray zone — almost half are denied. This group could benefit most from added support.\nLow (&lt;40%): Most were denied, but if any were income-eligible, this may indicate missed opportunities.\n\n\n\nGray Zone\nTo learn more about applicants in the 40–59% predicted range, I created a summary of their characteristics.\n\n\nCode\n# Get only the rows used in the model\nused_rows &lt;- as.numeric(rownames(model.frame(approval_model)))\n\n# Add predicted probabilities + other variables used in analysis\nmodel_data &lt;- exercise_data[used_rows, ] |&gt;\n  mutate(\n    predicted_prob = predict(approval_model, type = \"response\")\n  )\n\ngray_zone &lt;- model_data |&gt; \n  filter(predicted_prob &gt;= 0.4, predicted_prob &lt; 0.6)\n\ngray_zone_summary &lt;- gray_zone |&gt; \n  summarize(\n    `Number of People` = n(),\n    `Approval Rate` = round(mean(approved, na.rm = TRUE) * 100, 1),\n    `Pct. Income Eligible` = round(mean(income_eligible, na.rm = TRUE) * 100, 1),\n    `Pct. Docs With App` = round(mean(docs_with_app &gt; 0) * 100, 1),\n    `Pct. Docs Acfter App` = round(mean(docs_after_app &gt; 0) * 100, 1),\n    `Pct. Completed Interview` = round(mean(interview_completed == \"Completed\") * 100, 1)\n  )\n\ngray_zone_summary |&gt;\n  pivot_longer(everything()) |&gt;\n  gt::gt() |&gt;\n  gt::cols_label(\n    name = \"Metric\",\n    value = \"%\"\n  ) |&gt;\n  gt::fmt_number(columns = value, decimals = 1) |&gt;\n  fnc_style_gt_table()\n\n\n\n\n\n\n\n\nMetric\n%\n\n\n\n\nNumber of People\n316.0\n\n\nApproval Rate\n50.6\n\n\nPct. Income Eligible\n98.4\n\n\nPct. Docs With App\n46.5\n\n\nPct. Docs Acfter App\n24.4\n\n\nPct. Completed Interview\n31.3\n\n\n\n\n\n\n\nObservations:\n\nNearly half of the gray zone applicants were approved\n99% appear income-eligible, but:\n\nOnly 38% submitted documents with their application\nOnly 22% completed the interview\n\n\nThis group represents a major opportunity: they’re likely eligible, but many didn’t complete the full process. Small nudges or reminders could meaningfully increase approvals."
  },
  {
    "objectID": "analysis.html#conclusion",
    "href": "analysis.html#conclusion",
    "title": "Analysis Walkthrough",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis examined patterns in CalFresh (SNAP) application outcomes among GetCalFresh.org users in San Diego County. Several process-related factors were strongly associated with whether an applicant was approved.\n\nKey Findings:\n\nInterview completion was the most predictive factor: Applicants who reported completing the interview were nearly three times more likely to be approved. Their average predicted approval probability was 72%, compared to 50% for others.\nDocument uploads mattered: Uploading verification documents — especially with the initial application — was associated with higher approval rates.\nHigher income reduced the odds of approval: Each additional $500 in income was associated with about a one-third decrease in approval odds — even among mostly income-eligible applicants.\nMany income-eligible applicants were not approved: Nearly half of income-eligible applicants were denied, suggesting process-related barriers (e.g., missing interviews or documents) play a major role.\nZIP code predicted approval differences: Approval rates varied significantly by ZIP, pointing to geographic disparities in access or processing.\nA large group fell into a “gray zone”: Applicants with predicted approval probabilities between 40–59% were often income-eligible but missed key steps like interviews or document uploads. This group is a strong target for reminders or support."
  },
  {
    "objectID": "analysis.html#next-steps-areas-for-deeper-analysis",
    "href": "analysis.html#next-steps-areas-for-deeper-analysis",
    "title": "Analysis Walkthrough",
    "section": "Next Steps: Areas for Deeper Analysis",
    "text": "Next Steps: Areas for Deeper Analysis\nThis section outlines follow-up analyses and design considerations to expand on the current findings and inform future improvements to the CalFresh application process.\n\nGeographic and Neighborhood Variation\n\nLink ZIP codes to American Community Survey (ACS) indicators:\n\nPoverty rate\nHousing burden\nBroadband access\nLanguages spoken at home\n\nMap approval rates by neighborhood to identify areas with potential access barriers\nAssess whether geographic disparities persist after controlling for applicant characteristics\nImmigration status is requested in the survey, which may influence both applicant behavior and caseworker decisions. Consider whether areas with larger immigrant populations experience different approval rates, potentially due to documentation fears, interview accessibility, or language support gaps.\n\n\n\nQualitative Research\n\nInterview applicants to:\n\nUnderstand perceived barriers in the process\nIdentify confusing or unclear steps\nExplore unmet needs for documentation or interview follow-up\n\nReview SMS or helpdesk interactions for common pain points\n\n\n\nTiming and Process Flow\n\nAnalyze time between:\n\nApplication start and finish\nApplication and document upload\nApplication and interview completion\n\nIdentify drop-off points or common delays in the flow\nExplore whether earlier intervention (e.g., reminders) affects outcomes\n\n\n\nSystem and Policy Implications\n\nShare ZIP-level insights with county caseworkers and program administrators\nEvaluate platform changes (e.g., nudges, scheduling tools, help prompts) for impact on completion and approval\nExplore partnerships for assisted application support in low-approval ZIPs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nImproving Access to CalFresh\n",
    "section": "",
    "text": "Improving Access to CalFresh\n\n\nMaking food assistance easier to access, starting with the data.\n\nView Key Findings"
  }
]